{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNsd4dQu+ivOCNKiO9nBGAk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srikanth635/COMNETS/blob/main/Source_Code/JSONExtraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "1EL5OcmnI7fX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import pickle\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import scipy as sp\n",
        "from scipy import stats"
      ],
      "metadata": {
        "id": "USKCULdY6XaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Segmenting of Information from JSON file"
      ],
      "metadata": {
        "id": "SXQoU9bVI3mI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-ft0DAc5zIJ"
      },
      "outputs": [],
      "source": [
        "class json_segmentation:\n",
        "    \n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def function1(self,df):\n",
        "        #convert simulations column into series from original JSON\n",
        "        sim_df = df.simulations.apply(pd.Series)\n",
        "        #convert meta column into series from sim_df\n",
        "        sim_meta_df = sim_df.meta.apply(pd.Series)\n",
        "        #\n",
        "        sim_df.sim_state_times = sim_df.sim_state_times.apply(lambda x : json.loads(x))\n",
        "        sim_state_times_df = sim_df.sim_state_times.apply(pd.Series)\n",
        "        #\n",
        "        meta_scalar_columns = [content[0]+'_'+content[1] for index,content in enumerate(sim_meta_df.scalar_stats[0])]\n",
        "        #\n",
        "        matrix = np.zeros(len(meta_scalar_columns))\n",
        "        for i in range(len(sim_meta_df.scalar_stats)):\n",
        "            values = [content[2] for index,content in enumerate(sim_meta_df.scalar_stats[i])]\n",
        "            matrix = np.vstack((matrix,values))\n",
        "        #\n",
        "        meta_scalar_df = pd.DataFrame(data=matrix,columns=meta_scalar_columns)\n",
        "#         temp_df = meta_scalar_df.copy()\n",
        "        # index resetting --------\n",
        "        meta_scalar_df.drop(axis=0,index=0,inplace=True)\n",
        "        meta_scalar_df.reset_index(inplace=True)\n",
        "        meta_scalar_df.drop('index',axis=1,inplace=True)\n",
        "        #\n",
        "        meta_sim_timeStats_columns = [content[0] for index,content in enumerate(sim_meta_df.sim_runtime_stats[0])]\n",
        "        #\n",
        "        matrix1 = np.zeros(len(meta_sim_timeStats_columns))\n",
        "        for i in range(len(sim_meta_df.sim_runtime_stats)):\n",
        "            values = [content[1] for index,content in enumerate(sim_meta_df.sim_runtime_stats[i])]\n",
        "            matrix1 = np.vstack((matrix1,values))\n",
        "        #\n",
        "        meta_sim_timeStats_df = pd.DataFrame(data=matrix1,columns=meta_sim_timeStats_columns)\n",
        "        # index resetting --------\n",
        "        meta_sim_timeStats_df.drop(axis=0,index=0,inplace=True)\n",
        "        meta_sim_timeStats_df.reset_index(inplace=True)\n",
        "        meta_sim_timeStats_df.drop('index',axis=1,inplace=True)\n",
        "        #\n",
        "        sim_meta_df.drop(['scalar_stats','sim_runtime_stats'],axis=1,inplace=True)\n",
        "        sim_df.drop(['sim_state_times','meta'],axis=1,inplace=True)\n",
        "        # \n",
        "        df_merge = [sim_df,sim_meta_df,meta_scalar_df,meta_sim_timeStats_df,sim_state_times_df]\n",
        "        df_merge = [sim_df,sim_meta_df,meta_scalar_df,meta_sim_timeStats_df]\n",
        "        segregated_df = pd.concat(df_merge,axis=1)\n",
        "        \n",
        "        return segregated_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Information Extraction from segmented JSON"
      ],
      "metadata": {
        "id": "AHIOCqdEJNtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ini_feature_extraction:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def feature_separation(self,segmented_df):\n",
        "        #\n",
        "        feat_dict = {\n",
        "            'numNodes' : [],'dataGenerationInterval' : [],'dataSizeInBytes' : [],'constraintAreaMaxX' : [],'constraintAreaMaxY' : [],\n",
        "            'noOfLocations' : [],'Hosts' : [],'speed' : [],'forwardingLayer' : [],'maximumCacheSize' : [],'app_layer':[]\n",
        "                }\n",
        "        for index,row in segmented_df.iterrows():\n",
        "            strs = segmented_df['omnetppini'][index]\n",
        "            cur_runcon = segmented_df['runconfig'][index]\n",
        "#             runcon_list = re.findall(r'\\[([A-Za-z0-9\\s\\-]{2,})\\]',strs)\n",
        "  \n",
        "#             numNodes = re.compile(cur_runcon +r'[\\]\\n\\w\\s\\=\\-\\/\\\"\\$\\}\\[\\{\\.\\*\\d\\)\\(\\#\\+\\,\\\\n]+?numNodes = (\\d{1,})').findall(strs)\n",
        "            numNodes = re.compile(cur_runcon +r'[\\]\\n\\w\\s\\=\\-\\/\\\"\\$\\}\\{\\.\\*\\d\\)\\(\\#\\+\\,\\\\n]+numNodes = (\\d{1,})').findall(strs)\n",
        "            app_dataGenInterval = re.compile(cur_runcon +r'[\\]\\n\\w\\s\\=\\-\\/\\\"\\$\\}\\[\\{\\.\\*\\d\\)\\(\\#\\+\\,\\\\n]+?dataGenerationInterval = (\\d{1,})').findall(strs)\n",
        "#             app_dataSizeBytes = re.compile(cur_runcon +r'[\\]\\n\\w\\s\\=\\-\\/\\\"\\$\\}\\[\\{\\.\\*\\d\\)\\(\\#\\+\\,\\\\n]+?dataSizeInBytes = (\\d{1,})').findall(strs)        \n",
        "            app_dataSizeBytes = re.compile(cur_runcon +r'[\\]\\n\\w\\s\\=\\-\\/\\\"\\$\\}\\{\\.\\*\\d\\)\\(\\#\\+\\,\\\\n]+dataSizeInBytes = (\\d{1,})').findall(strs)        \n",
        "            mob_area_maxX = re.compile(cur_runcon +r'[\\]\\n\\w\\s\\=\\-\\/\\\"\\$\\}\\[\\{\\.\\*\\d\\)\\(\\#\\+\\,\\\\n]+?constraintAreaMaxX = (\\d{1,})').findall(strs)\n",
        "            mob_area_maxY = re.compile(cur_runcon +r'[\\]\\n\\w\\s\\=\\-\\/\\\"\\$\\}\\[\\{\\.\\*\\d\\)\\(\\#\\+\\,\\\\n]+?constraintAreaMaxY = (\\d{1,})').findall(strs)           \n",
        "            mob_locations = re.compile(cur_runcon +r'[\\]\\n\\w\\s\\=\\-\\/\\\"\\$\\}\\[\\{\\.\\*\\d\\)\\(\\#\\+\\,\\\\n]+?noOfLocations = (\\d{1,})').findall(strs)\n",
        "            mob_hosts = re.compile(cur_runcon +r'[\\]\\n\\w\\s\\=\\-\\/\\\"\\$\\}\\[\\{\\.\\*\\d\\)\\(\\#\\+\\,\\\\n]+?Hosts = (\\d{1,})').findall(strs)\n",
        "            mob_speed = re.compile(cur_runcon +r'[\\]\\n\\w\\s\\=\\-\\/\\\"\\$\\}\\[\\{\\.\\*\\d\\)\\(\\#\\+\\,\\\\n]+?speed = (\\d{1,}\\.\\d{1,}|\\d{1,})').findall(strs)\n",
        "           \n",
        "            #forwarding\n",
        "            forw_layer = re.compile(cur_runcon +r'[\\]\\n\\w\\s\\=\\-\\/\\\"\\$\\}\\[\\{\\.\\*\\d\\)\\(\\#\\+\\,\\\\n]+?forwardingLayer = \\\"([A-Za-z]+)\\\"').findall(strs)\n",
        "#             maximumCacheSize = re.compile(cur_runcon +r'[\\]\\n\\w\\s\\=\\-\\/\\\"\\$\\}\\[\\{\\.\\*\\d\\)\\(\\#\\+\\,\\\\n]+?maximumCacheSize = (\\d+)').findall(strs)\n",
        "            maximumCacheSize = re.compile(cur_runcon +r'[\\]\\n\\w\\s\\=\\-\\/\\\"\\$\\}\\{\\.\\*\\d\\)\\(\\#\\+\\,\\\\n]+maximumCacheSize = (\\d+)').findall(strs)\n",
        "            \n",
        "            app_layer = re.compile(cur_runcon +r'[\\]\\n\\w\\s\\=\\-\\/\\\"\\$\\}\\{\\.\\*\\d\\)\\(\\#\\+\\,\\\\n]+applicationLayer = \"([\\w]+)\"').findall(strs)\n",
        "\n",
        "            if len(numNodes)==0:\n",
        "                numNodes = re.compile(r'numNodes = (\\d+)').findall(strs)\n",
        "                \n",
        "            if bool(numNodes):\n",
        "                feat_dict['numNodes'].append(numNodes[0])\n",
        "            else:\n",
        "                feat_dict['numNodes'].append(0)\n",
        "                \n",
        "            if bool(app_dataGenInterval):\n",
        "                feat_dict['dataGenerationInterval'].append(app_dataGenInterval[0])\n",
        "            else:\n",
        "                feat_dict['dataGenerationInterval'].append(0)\n",
        "                \n",
        "            if bool(app_dataSizeBytes):\n",
        "                feat_dict['dataSizeInBytes'].append(app_dataSizeBytes[0])\n",
        "            else:\n",
        "                feat_dict['dataSizeInBytes'].append(0)\n",
        "                \n",
        "            if bool(mob_area_maxX):\n",
        "                feat_dict['constraintAreaMaxX'].append(mob_area_maxX[0])\n",
        "            else:\n",
        "                feat_dict['constraintAreaMaxX'].append(0)\n",
        "                \n",
        "            if bool(mob_area_maxY):\n",
        "                feat_dict['constraintAreaMaxY'].append(mob_area_maxY[0])\n",
        "            else:\n",
        "                feat_dict['constraintAreaMaxY'].append(0)\n",
        "                \n",
        "            if bool(mob_locations):\n",
        "                feat_dict['noOfLocations'].append(mob_locations[0])\n",
        "            else:\n",
        "                feat_dict['noOfLocations'].append(0)\n",
        "                \n",
        "            if bool(mob_hosts):\n",
        "                feat_dict['Hosts'].append(mob_hosts[0])\n",
        "            else:\n",
        "                feat_dict['Hosts'].append(0)\n",
        "                \n",
        "            if bool(mob_speed):\n",
        "                feat_dict['speed'].append(mob_speed[0])\n",
        "            else:\n",
        "                feat_dict['speed'].append(0)\n",
        "                \n",
        "            if bool(forw_layer): \n",
        "                feat_dict['forwardingLayer'].append(forw_layer[0])\n",
        "            else:\n",
        "                feat_dict['forwardingLayer'].append(0)\n",
        "\n",
        "            if bool(maximumCacheSize): \n",
        "                feat_dict['maximumCacheSize'].append(maximumCacheSize[0])\n",
        "            else:\n",
        "                feat_dict['maximumCacheSize'].append(0)\n",
        "                \n",
        "            if bool(app_layer): \n",
        "                feat_dict['app_layer'].append(app_layer[0])\n",
        "            else:\n",
        "                feat_dict['app_layer'].append(0)\n",
        "        return feat_dict"
      ],
      "metadata": {
        "id": "CChr9dEb6C0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting the extracted infomration into pandas dataframe"
      ],
      "metadata": {
        "id": "S_ITfaARJWJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class dataSetGenerator:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def dataSetGen(self,df_to_segment):\n",
        "        bms = ['General','Config Benchmark-04-Conference-Scenario','Config Benchmark-07-Office-Scenario','Config Benchmark-05-University-Scenario',\n",
        "          'Config Benchmark-06-Roller-Skate-Scenario','Config Benchmark-02-Typhoon-Disaster-Scenario']\n",
        "        target_df = df_to_segment[['runconfig','peak_disk_usage', 'peak_sim_ram_usage','peak_results_ram_usage','totaljobclocktimesec']]\n",
        "        new_segmented_df = df_to_segment[~df_to_segment.runconfig.isin(bms)].reset_index(drop=True)\n",
        "        new_target_df = target_df[~target_df.runconfig.isin(bms)].reset_index(drop=True)\n",
        "        print(\"Segments : \",new_segmented_df.shape,new_target_df.shape)\n",
        "        new_ini_obj = ini_feature_extraction()\n",
        "        feat_xtrac = new_ini_obj.feature_separation(new_segmented_df)\n",
        "        new_feature_df = pd.DataFrame(feat_xtrac)\n",
        "        full_df = pd.concat([new_feature_df,new_target_df],axis=1)\n",
        "        final_df = full_df.drop(full_df[(full_df.Hosts==0)|(full_df.peak_results_ram_usage==0)|\n",
        "                                        (full_df.peak_sim_ram_usage==0)|(full_df.peak_disk_usage==0)|(full_df.constraintAreaMaxX==0)|(full_df.constraintAreaMaxY==0)].index)\n",
        "        final_df = final_df.drop('speed',axis=1)\n",
        "        object_list = ['numNodes', 'dataGenerationInterval', 'dataSizeInBytes','constraintAreaMaxX','constraintAreaMaxY', \n",
        "       'noOfLocations', 'Hosts', 'maximumCacheSize','totaljobclocktimesec']\n",
        "        final_df[object_list] = final_df[object_list].apply(pd.to_numeric)\n",
        "        \n",
        "       \n",
        "        print(\"Final df shape : \", final_df.shape)\n",
        "        return final_df"
      ],
      "metadata": {
        "id": "AQOGG3pH6HMn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}